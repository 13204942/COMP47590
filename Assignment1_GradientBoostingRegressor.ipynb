{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: Implementing Gradient Boosting - SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Student 1 Name:   Fangyijie Wang\n",
    "- Student 1 Number: 13204942"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "import sklearn\n",
    "import sklearn.utils\n",
    "import sklearn.model_selection\n",
    "import sklearn.base \n",
    "import sklearn.utils.validation \n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: The GradientBoosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the GradientBoostingRegressor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and RegressorMixin classes\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"GradientBoostingRegressor only works for continuous descriptive features and continuous target features.  \n",
    "        - Training: Train the new model to add to the ensemble by training the model to predict the errors (in regression terms the residuals) of the old model.\n",
    "        - Prediction: When a new prediction needs to be made compare the descriptive feature values of the new query instance to each tree and return the target feature that is close to the query case. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_base_estimators numeric, required (default = 1) \n",
    "        The number of estimators to include in the model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    trees_: dict\n",
    "        A dictionary of the DecisionTree on all descriptive features in each epoch.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> reg = GradientBoostingRegressor()\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(reg, iris.data, iris.target, cv=10)\n",
    "    \"\"\"\n",
    "    # Constructor for the regressor object\n",
    "    def __init__(self, \n",
    "                 num_base_estimators, # the number of base estimators to include\n",
    "                 tree_depth=5,  # the depth of decision tree\n",
    "                 min_samples_leaf=1, # the mini number of samples required to be at a leaf node\n",
    "                 random_state=None\n",
    "                ):\n",
    "        self.num_base_estimators = num_base_estimators\n",
    "        self.tree_depth = tree_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.random_state = random_state\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build an educated guess regressor from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csc_matrix``.\n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        \n",
    "        # change instance from array to 2D matrix\n",
    "        X = np.array(X)      \n",
    "        y = np.array(y)       \n",
    "            \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        # Set up the random number generator to be used to generate \n",
    "        # predictions - this follows reccommended scikitlearn pattern\n",
    "        # https://scikit-learn.org/stable/developers/develop.html#coding-guidelines\n",
    "        self.random_state_ = check_random_state(self.random_state)\n",
    "        \n",
    "        self.trees_ = []\n",
    "        \n",
    "        self.m0 = y.mean()\n",
    "        M = self.m0\n",
    "        \n",
    "        for i in range(self.num_base_estimators):\n",
    "            # compute residuals\n",
    "            res = y - self.m0\n",
    "            # fit DecisionTree on X and residuals\n",
    "            tree = sklearn.tree.DecisionTreeRegressor(max_depth=self.tree_depth,\n",
    "                                                      min_samples_leaf=self.min_samples_leaf,\n",
    "                                                      random_state=self.random_state)\n",
    "            tree.fit(X, res)\n",
    "            delta = tree.predict(X)\n",
    "            # add delta to the model M \n",
    "            M = M + delta\n",
    "            # append the tree\n",
    "            self.trees_.append(tree)\n",
    "            \n",
    "        # Return the regressor\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict continues value of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check is fit had been called by confirming that the trees_ dictionary has been set up\n",
    "        check_is_fitted(self, ['trees_'])\n",
    "        \n",
    "        # change instance from array to 2D matrix\n",
    "        X = np.array(X)             \n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        M = self.m0\n",
    "        \n",
    "        for i in range(self.num_base_estimators):\n",
    "            # use trees to predict target feature\n",
    "            M = M + self.trees_[i].predict(X)\n",
    "            \n",
    "        return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some dictionaries to store simple model performance comparions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_valid_RMSE_comparisons = dict()\n",
    "model_RMSE_comparisons = dict()\n",
    "model_tuned_params_list = dict()\n",
    "cvfolds = 10\n",
    "# use Root Mean Square Error (RMSE) as the evaluation metric\n",
    "scoring = 'neg_root_mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Readability Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 200) \n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_file = 'commonlitreadabilityprize_bow.csv'\n",
    "embedding_file = 'commonlitreadabilityprize_embedding.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2834, 1829)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>$</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accompanied</th>\n",
       "      <th>accomplished</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>acid</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>active</th>\n",
       "      <th>activity</th>\n",
       "      <th>acts</th>\n",
       "      <th>actual</th>\n",
       "      <th>actually</th>\n",
       "      <th>added</th>\n",
       "      <th>addition</th>\n",
       "      <th>admiration</th>\n",
       "      <th>advanced</th>\n",
       "      <th>advantage</th>\n",
       "      <th>affairs</th>\n",
       "      <th>afraid</th>\n",
       "      <th>africa</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>afterward</th>\n",
       "      <th>age</th>\n",
       "      <th>ages</th>\n",
       "      <th>ago</th>\n",
       "      <th>agreed</th>\n",
       "      <th>ah</th>\n",
       "      <th>ahead</th>\n",
       "      <th>aid</th>\n",
       "      <th>air</th>\n",
       "      <th>alice</th>\n",
       "      <th>allow</th>\n",
       "      <th>allowed</th>\n",
       "      <th>allows</th>\n",
       "      <th>america</th>\n",
       "      <th>american</th>\n",
       "      <th>americans</th>\n",
       "      <th>ancient</th>\n",
       "      <th>angry</th>\n",
       "      <th>animal</th>\n",
       "      <th>animals</th>\n",
       "      <th>answer</th>\n",
       "      <th>answered</th>\n",
       "      <th>anxious</th>\n",
       "      <th>apart</th>\n",
       "      <th>...</th>\n",
       "      <th>window</th>\n",
       "      <th>windows</th>\n",
       "      <th>winds</th>\n",
       "      <th>wing</th>\n",
       "      <th>wings</th>\n",
       "      <th>winter</th>\n",
       "      <th>wire</th>\n",
       "      <th>wise</th>\n",
       "      <th>wish</th>\n",
       "      <th>wished</th>\n",
       "      <th>wo</th>\n",
       "      <th>woke</th>\n",
       "      <th>woman</th>\n",
       "      <th>women</th>\n",
       "      <th>won</th>\n",
       "      <th>wonder</th>\n",
       "      <th>wondered</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>wood</th>\n",
       "      <th>wooden</th>\n",
       "      <th>woods</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>wore</th>\n",
       "      <th>work</th>\n",
       "      <th>worked</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>worn</th>\n",
       "      <th>worried</th>\n",
       "      <th>worse</th>\n",
       "      <th>worth</th>\n",
       "      <th>write</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>yard</th>\n",
       "      <th>yards</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>°</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c12129c31</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>-0.315372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>-0.580118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>-1.054013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>0.247197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1829 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  $  ability  able  absolutely  accompanied  accomplished  \\\n",
       "0           0  0        0     0           0            0             0   \n",
       "1           1  0        0     0           0            0             0   \n",
       "2           2  0        0     0           0            0             0   \n",
       "3           3  0        0     0           0            0             0   \n",
       "4           4  0        0     0           0            0             0   \n",
       "\n",
       "   according  account  acid  act  action  active  activity  acts  actual  \\\n",
       "0          0        0     0    0       0       0         0     0       0   \n",
       "1          0        0     0    0       0       0         0     0       0   \n",
       "2          0        0     0    0       0       0         0     0       0   \n",
       "3          0        0     0    0       0       0         0     0       0   \n",
       "4          0        0     0    0       0       0         0     0       0   \n",
       "\n",
       "   actually  added  addition  admiration  advanced  advantage  affairs  \\\n",
       "0         0      0         0           0         0          0        0   \n",
       "1         0      0         0           0         0          0        0   \n",
       "2         0      0         0           0         0          0        0   \n",
       "3         0      0         0           0         0          0        0   \n",
       "4         0      0         0           0         0          0        0   \n",
       "\n",
       "   afraid  africa  afternoon  afterward  age  ages  ago  agreed  ah  ahead  \\\n",
       "0       0       0          0          0    0     0    0       0   0      0   \n",
       "1       0       0          0          0    0     0    0       0   0      0   \n",
       "2       0       0          0          0    0     0    0       0   0      0   \n",
       "3       0       0          0          0    0     0    0       0   0      0   \n",
       "4       0       0          0          0    0     0    0       0   0      0   \n",
       "\n",
       "   aid  air  alice  allow  allowed  allows  america  american  americans  \\\n",
       "0    0    0      0      0        0       0        0         0          0   \n",
       "1    0    0      0      0        0       0        0         0          0   \n",
       "2    0    0      0      0        0       0        0         0          0   \n",
       "3    0    0      0      0        0       0        0         0          0   \n",
       "4    0    0      0      0        0       0        0         0          0   \n",
       "\n",
       "   ancient  angry  animal  animals  answer  answered  anxious  apart  ...  \\\n",
       "0        0      0       0        0       0         0        0      0  ...   \n",
       "1        0      0       0        0       0         0        0      0  ...   \n",
       "2        0      0       0        0       0         0        0      0  ...   \n",
       "3        0      0       0        0       0         0        0      0  ...   \n",
       "4        0      0       0        0       0         0        0      0  ...   \n",
       "\n",
       "   window  windows  winds  wing  wings  winter  wire  wise  wish  wished  wo  \\\n",
       "0       0        0      0     0      0       1     0     0     0       0   0   \n",
       "1       0        0      0     0      0       0     0     0     0       0   0   \n",
       "2       0        0      0     0      0       0     0     0     0       0   0   \n",
       "3       0        0      0     0      0       1     0     2     0       0   0   \n",
       "4       0        0      0     0      0       0     0     0     0       0   0   \n",
       "\n",
       "   woke  woman  women  won  wonder  wondered  wonderful  wood  wooden  woods  \\\n",
       "0     0      0      0    0       0         0          0     0       0      0   \n",
       "1     0      0      0    0       0         0          0     0       0      0   \n",
       "2     0      0      0    0       0         0          0     0       0      0   \n",
       "3     0      0      0    0       0         0          0     0       0      0   \n",
       "4     0      0      0    0       0         0          0     1       0      0   \n",
       "\n",
       "   word  words  wore  work  worked  working  works  world  worn  worried  \\\n",
       "0     0      0     0     0       0        0      0      0     0        0   \n",
       "1     0      0     0     1       0        0      0      0     0        0   \n",
       "2     0      0     0     0       0        0      0      0     0        0   \n",
       "3     0      0     0     0       0        0      0      0     0        0   \n",
       "4     0      0     0     0       0        0      0      0     0        0   \n",
       "\n",
       "   worse  worth  write  writing  written  wrong  wrote  yard  yards  year  \\\n",
       "0      0      0      0        0        0      0      0     0      0     0   \n",
       "1      0      0      0        0        0      0      0     0      0     0   \n",
       "2      0      0      0        0        0      0      0     0      0     0   \n",
       "3      0      0      0        0        0      0      0     0      0     2   \n",
       "4      0      0      0        0        0      0      0     0      0     0   \n",
       "\n",
       "   years  yellow  yes  york  young  youth  °         id    target  \n",
       "0      0       0    0     0      1      0  0  c12129c31 -0.340259  \n",
       "1      0       0    0     0      0      0  0  85aa80a4c -0.315372  \n",
       "2      0       0    0     0      0      0  0  b69ac6792 -0.580118  \n",
       "3      0       0    0     0      0      0  0  dd1000b26 -1.054013  \n",
       "4      0       0    0     0      0      0  0  37c1b32fb  0.247197  \n",
       "\n",
       "[5 rows x 1829 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bow_training_data = pd.read_csv(bag_of_words_file)\n",
    "print(bow_training_data.shape)\n",
    "display(bow_training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values\n",
      "Unnamed: 0    0\n",
      "produce       0\n",
      "process       0\n",
      "problems      0\n",
      "problem       0\n",
      "             ..\n",
      "follows       0\n",
      "following     0\n",
      "followed      0\n",
      "follow        0\n",
      "target        0\n",
      "Length: 1829, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing Values\")\n",
    "print(bow_training_data.isnull().sum().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2834, 303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>...</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.054942</td>\n",
       "      <td>0.104693</td>\n",
       "      <td>-0.129839</td>\n",
       "      <td>-0.077362</td>\n",
       "      <td>0.066479</td>\n",
       "      <td>-0.007780</td>\n",
       "      <td>-0.010820</td>\n",
       "      <td>-0.054412</td>\n",
       "      <td>-0.069162</td>\n",
       "      <td>2.085748</td>\n",
       "      <td>-0.060080</td>\n",
       "      <td>0.040206</td>\n",
       "      <td>0.078970</td>\n",
       "      <td>-0.092061</td>\n",
       "      <td>-0.113453</td>\n",
       "      <td>-0.105683</td>\n",
       "      <td>-0.018003</td>\n",
       "      <td>1.089996</td>\n",
       "      <td>-0.157291</td>\n",
       "      <td>-0.049872</td>\n",
       "      <td>-0.038974</td>\n",
       "      <td>-0.044448</td>\n",
       "      <td>-0.051416</td>\n",
       "      <td>0.072666</td>\n",
       "      <td>0.006214</td>\n",
       "      <td>-0.023308</td>\n",
       "      <td>-0.103280</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>-0.035179</td>\n",
       "      <td>-0.041778</td>\n",
       "      <td>-0.010981</td>\n",
       "      <td>0.052993</td>\n",
       "      <td>-0.096132</td>\n",
       "      <td>0.057582</td>\n",
       "      <td>0.072257</td>\n",
       "      <td>-0.147485</td>\n",
       "      <td>-0.058766</td>\n",
       "      <td>-0.006197</td>\n",
       "      <td>-0.030359</td>\n",
       "      <td>-0.055719</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>0.133593</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>-0.103093</td>\n",
       "      <td>0.096608</td>\n",
       "      <td>0.020460</td>\n",
       "      <td>-0.124611</td>\n",
       "      <td>-0.101497</td>\n",
       "      <td>0.044355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>-0.022423</td>\n",
       "      <td>0.052422</td>\n",
       "      <td>0.258125</td>\n",
       "      <td>0.172970</td>\n",
       "      <td>0.005742</td>\n",
       "      <td>-0.068247</td>\n",
       "      <td>0.043332</td>\n",
       "      <td>0.175139</td>\n",
       "      <td>0.131109</td>\n",
       "      <td>-0.030145</td>\n",
       "      <td>0.030684</td>\n",
       "      <td>0.077186</td>\n",
       "      <td>-0.009650</td>\n",
       "      <td>-0.001977</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>0.067259</td>\n",
       "      <td>-0.036809</td>\n",
       "      <td>-0.027976</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>-0.074472</td>\n",
       "      <td>-0.165668</td>\n",
       "      <td>-0.128944</td>\n",
       "      <td>-0.001251</td>\n",
       "      <td>0.025481</td>\n",
       "      <td>0.116750</td>\n",
       "      <td>-0.038316</td>\n",
       "      <td>0.141899</td>\n",
       "      <td>0.243866</td>\n",
       "      <td>0.032853</td>\n",
       "      <td>-0.060356</td>\n",
       "      <td>-0.101602</td>\n",
       "      <td>-0.050094</td>\n",
       "      <td>-0.060763</td>\n",
       "      <td>0.114494</td>\n",
       "      <td>-0.052732</td>\n",
       "      <td>0.121442</td>\n",
       "      <td>-0.079718</td>\n",
       "      <td>-0.238603</td>\n",
       "      <td>0.031822</td>\n",
       "      <td>0.059434</td>\n",
       "      <td>-0.093304</td>\n",
       "      <td>-0.134071</td>\n",
       "      <td>0.053603</td>\n",
       "      <td>0.038264</td>\n",
       "      <td>-0.028437</td>\n",
       "      <td>-0.022459</td>\n",
       "      <td>0.068514</td>\n",
       "      <td>c12129c31</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.014731</td>\n",
       "      <td>0.213879</td>\n",
       "      <td>-0.183907</td>\n",
       "      <td>-0.048958</td>\n",
       "      <td>0.112992</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.028143</td>\n",
       "      <td>-0.137892</td>\n",
       "      <td>-0.094396</td>\n",
       "      <td>2.170374</td>\n",
       "      <td>-0.155052</td>\n",
       "      <td>0.084018</td>\n",
       "      <td>0.093432</td>\n",
       "      <td>-0.090761</td>\n",
       "      <td>-0.184210</td>\n",
       "      <td>-0.045192</td>\n",
       "      <td>-0.100942</td>\n",
       "      <td>0.870650</td>\n",
       "      <td>-0.183582</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>-0.002261</td>\n",
       "      <td>-0.022219</td>\n",
       "      <td>-0.042072</td>\n",
       "      <td>-0.042623</td>\n",
       "      <td>-0.055870</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>-0.014145</td>\n",
       "      <td>-0.076327</td>\n",
       "      <td>0.112394</td>\n",
       "      <td>-0.168088</td>\n",
       "      <td>-0.052931</td>\n",
       "      <td>0.045638</td>\n",
       "      <td>-0.100293</td>\n",
       "      <td>0.127503</td>\n",
       "      <td>0.101659</td>\n",
       "      <td>-0.109052</td>\n",
       "      <td>-0.011360</td>\n",
       "      <td>0.030617</td>\n",
       "      <td>-0.090257</td>\n",
       "      <td>-0.035377</td>\n",
       "      <td>-0.006508</td>\n",
       "      <td>-0.010191</td>\n",
       "      <td>-0.032645</td>\n",
       "      <td>-0.120629</td>\n",
       "      <td>0.083803</td>\n",
       "      <td>0.067324</td>\n",
       "      <td>-0.196460</td>\n",
       "      <td>-0.060567</td>\n",
       "      <td>0.115225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112212</td>\n",
       "      <td>-0.036007</td>\n",
       "      <td>0.033733</td>\n",
       "      <td>0.198936</td>\n",
       "      <td>0.076721</td>\n",
       "      <td>-0.052154</td>\n",
       "      <td>-0.087582</td>\n",
       "      <td>0.107428</td>\n",
       "      <td>0.068410</td>\n",
       "      <td>0.107855</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>0.111956</td>\n",
       "      <td>0.051873</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>-0.014444</td>\n",
       "      <td>0.032021</td>\n",
       "      <td>0.122599</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>-0.031341</td>\n",
       "      <td>-0.088206</td>\n",
       "      <td>-0.100880</td>\n",
       "      <td>-0.094794</td>\n",
       "      <td>-0.113471</td>\n",
       "      <td>0.018583</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.077720</td>\n",
       "      <td>0.028152</td>\n",
       "      <td>0.201130</td>\n",
       "      <td>0.212862</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.066036</td>\n",
       "      <td>-0.129031</td>\n",
       "      <td>-0.026765</td>\n",
       "      <td>-0.049921</td>\n",
       "      <td>0.120074</td>\n",
       "      <td>-0.079767</td>\n",
       "      <td>0.147799</td>\n",
       "      <td>0.018992</td>\n",
       "      <td>-0.105756</td>\n",
       "      <td>0.051830</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>-0.007093</td>\n",
       "      <td>0.065427</td>\n",
       "      <td>-0.029093</td>\n",
       "      <td>-0.000948</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>-0.315372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.006671</td>\n",
       "      <td>0.217069</td>\n",
       "      <td>-0.125178</td>\n",
       "      <td>-0.073087</td>\n",
       "      <td>0.106584</td>\n",
       "      <td>0.016715</td>\n",
       "      <td>0.011751</td>\n",
       "      <td>-0.115226</td>\n",
       "      <td>-0.071845</td>\n",
       "      <td>2.141033</td>\n",
       "      <td>-0.135347</td>\n",
       "      <td>0.044908</td>\n",
       "      <td>0.057914</td>\n",
       "      <td>-0.058195</td>\n",
       "      <td>-0.149449</td>\n",
       "      <td>-0.053666</td>\n",
       "      <td>-0.026909</td>\n",
       "      <td>0.822324</td>\n",
       "      <td>-0.158412</td>\n",
       "      <td>-0.001502</td>\n",
       "      <td>-0.007176</td>\n",
       "      <td>-0.028327</td>\n",
       "      <td>-0.014629</td>\n",
       "      <td>-0.048186</td>\n",
       "      <td>0.021881</td>\n",
       "      <td>0.019163</td>\n",
       "      <td>-0.091954</td>\n",
       "      <td>-0.019096</td>\n",
       "      <td>0.069852</td>\n",
       "      <td>-0.100524</td>\n",
       "      <td>-0.051520</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>-0.067022</td>\n",
       "      <td>0.044913</td>\n",
       "      <td>0.044163</td>\n",
       "      <td>-0.114472</td>\n",
       "      <td>-0.039097</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>-0.076740</td>\n",
       "      <td>-0.006505</td>\n",
       "      <td>-0.006603</td>\n",
       "      <td>0.046618</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>-0.098088</td>\n",
       "      <td>0.080918</td>\n",
       "      <td>0.063744</td>\n",
       "      <td>-0.212404</td>\n",
       "      <td>-0.022673</td>\n",
       "      <td>0.051141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064818</td>\n",
       "      <td>-0.047286</td>\n",
       "      <td>0.058058</td>\n",
       "      <td>0.155161</td>\n",
       "      <td>0.062122</td>\n",
       "      <td>-0.060407</td>\n",
       "      <td>-0.057822</td>\n",
       "      <td>0.058438</td>\n",
       "      <td>0.152636</td>\n",
       "      <td>0.142695</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.106070</td>\n",
       "      <td>0.059557</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>-0.027812</td>\n",
       "      <td>-0.003158</td>\n",
       "      <td>0.136810</td>\n",
       "      <td>0.004483</td>\n",
       "      <td>0.043852</td>\n",
       "      <td>-0.073878</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>-0.103589</td>\n",
       "      <td>-0.124074</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.018385</td>\n",
       "      <td>0.057015</td>\n",
       "      <td>-0.003170</td>\n",
       "      <td>0.170545</td>\n",
       "      <td>0.187558</td>\n",
       "      <td>0.017306</td>\n",
       "      <td>-0.009738</td>\n",
       "      <td>-0.094571</td>\n",
       "      <td>-0.017686</td>\n",
       "      <td>-0.045320</td>\n",
       "      <td>0.086171</td>\n",
       "      <td>-0.079157</td>\n",
       "      <td>0.120796</td>\n",
       "      <td>-0.057970</td>\n",
       "      <td>-0.116192</td>\n",
       "      <td>0.020743</td>\n",
       "      <td>0.012045</td>\n",
       "      <td>-0.028818</td>\n",
       "      <td>0.044761</td>\n",
       "      <td>-0.033804</td>\n",
       "      <td>0.015294</td>\n",
       "      <td>-0.019791</td>\n",
       "      <td>0.020420</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>-0.580118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.040802</td>\n",
       "      <td>0.123908</td>\n",
       "      <td>-0.106365</td>\n",
       "      <td>-0.109637</td>\n",
       "      <td>0.090377</td>\n",
       "      <td>0.020302</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.007752</td>\n",
       "      <td>-0.103293</td>\n",
       "      <td>2.044155</td>\n",
       "      <td>-0.114577</td>\n",
       "      <td>0.053231</td>\n",
       "      <td>-0.004328</td>\n",
       "      <td>-0.138667</td>\n",
       "      <td>-0.142876</td>\n",
       "      <td>-0.056131</td>\n",
       "      <td>-0.052775</td>\n",
       "      <td>0.946257</td>\n",
       "      <td>-0.108830</td>\n",
       "      <td>-0.033371</td>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.036760</td>\n",
       "      <td>-0.107292</td>\n",
       "      <td>0.045815</td>\n",
       "      <td>-0.012851</td>\n",
       "      <td>-0.082862</td>\n",
       "      <td>-0.054655</td>\n",
       "      <td>0.042623</td>\n",
       "      <td>-0.008409</td>\n",
       "      <td>-0.040737</td>\n",
       "      <td>-0.051928</td>\n",
       "      <td>0.110341</td>\n",
       "      <td>-0.107192</td>\n",
       "      <td>0.056082</td>\n",
       "      <td>0.109184</td>\n",
       "      <td>-0.118366</td>\n",
       "      <td>-0.025728</td>\n",
       "      <td>-0.035033</td>\n",
       "      <td>-0.067925</td>\n",
       "      <td>0.029779</td>\n",
       "      <td>-0.050414</td>\n",
       "      <td>0.064762</td>\n",
       "      <td>0.012224</td>\n",
       "      <td>-0.119392</td>\n",
       "      <td>0.147026</td>\n",
       "      <td>0.033925</td>\n",
       "      <td>-0.105274</td>\n",
       "      <td>-0.068832</td>\n",
       "      <td>0.058932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029999</td>\n",
       "      <td>-0.057206</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.267056</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>-0.019701</td>\n",
       "      <td>-0.091312</td>\n",
       "      <td>0.085022</td>\n",
       "      <td>0.129807</td>\n",
       "      <td>0.108902</td>\n",
       "      <td>-0.002623</td>\n",
       "      <td>0.118156</td>\n",
       "      <td>0.087849</td>\n",
       "      <td>-0.070147</td>\n",
       "      <td>-0.043028</td>\n",
       "      <td>-0.082752</td>\n",
       "      <td>-0.061191</td>\n",
       "      <td>-0.065372</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>-0.036547</td>\n",
       "      <td>-0.018116</td>\n",
       "      <td>-0.131600</td>\n",
       "      <td>-0.127061</td>\n",
       "      <td>0.053523</td>\n",
       "      <td>0.026109</td>\n",
       "      <td>0.114862</td>\n",
       "      <td>-0.014734</td>\n",
       "      <td>0.143633</td>\n",
       "      <td>0.312614</td>\n",
       "      <td>0.014281</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>-0.085322</td>\n",
       "      <td>-0.002954</td>\n",
       "      <td>-0.006855</td>\n",
       "      <td>0.155161</td>\n",
       "      <td>-0.058557</td>\n",
       "      <td>0.134907</td>\n",
       "      <td>-0.142102</td>\n",
       "      <td>-0.201305</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>-0.002538</td>\n",
       "      <td>-0.046069</td>\n",
       "      <td>-0.155320</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.079673</td>\n",
       "      <td>-0.035276</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.066118</td>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>-1.054013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.018610</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>-0.161603</td>\n",
       "      <td>-0.035534</td>\n",
       "      <td>0.143024</td>\n",
       "      <td>-0.053699</td>\n",
       "      <td>-0.026999</td>\n",
       "      <td>-0.152680</td>\n",
       "      <td>-0.012829</td>\n",
       "      <td>2.205195</td>\n",
       "      <td>-0.216011</td>\n",
       "      <td>0.017377</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>-0.137153</td>\n",
       "      <td>-0.142915</td>\n",
       "      <td>-0.038127</td>\n",
       "      <td>-0.059814</td>\n",
       "      <td>1.205517</td>\n",
       "      <td>-0.104666</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>-0.049459</td>\n",
       "      <td>-0.069475</td>\n",
       "      <td>-0.073464</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.027229</td>\n",
       "      <td>-0.035814</td>\n",
       "      <td>-0.045660</td>\n",
       "      <td>0.016310</td>\n",
       "      <td>0.109001</td>\n",
       "      <td>-0.021372</td>\n",
       "      <td>-0.066788</td>\n",
       "      <td>0.066605</td>\n",
       "      <td>-0.080589</td>\n",
       "      <td>0.068447</td>\n",
       "      <td>0.259053</td>\n",
       "      <td>-0.222512</td>\n",
       "      <td>0.062160</td>\n",
       "      <td>-0.022953</td>\n",
       "      <td>-0.111141</td>\n",
       "      <td>-0.081637</td>\n",
       "      <td>0.030493</td>\n",
       "      <td>0.199560</td>\n",
       "      <td>-0.042170</td>\n",
       "      <td>-0.110937</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>0.065180</td>\n",
       "      <td>-0.122961</td>\n",
       "      <td>-0.050939</td>\n",
       "      <td>0.069089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077576</td>\n",
       "      <td>0.034792</td>\n",
       "      <td>0.051210</td>\n",
       "      <td>0.306868</td>\n",
       "      <td>0.168020</td>\n",
       "      <td>-0.054156</td>\n",
       "      <td>-0.084295</td>\n",
       "      <td>0.114701</td>\n",
       "      <td>0.031023</td>\n",
       "      <td>0.171025</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>0.077978</td>\n",
       "      <td>0.072445</td>\n",
       "      <td>-0.051332</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.014550</td>\n",
       "      <td>0.057516</td>\n",
       "      <td>-0.060990</td>\n",
       "      <td>0.019451</td>\n",
       "      <td>-0.079645</td>\n",
       "      <td>-0.029432</td>\n",
       "      <td>-0.204868</td>\n",
       "      <td>-0.190533</td>\n",
       "      <td>-0.024177</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.121633</td>\n",
       "      <td>0.014271</td>\n",
       "      <td>0.166662</td>\n",
       "      <td>0.304888</td>\n",
       "      <td>0.092535</td>\n",
       "      <td>-0.067335</td>\n",
       "      <td>-0.114079</td>\n",
       "      <td>-0.040758</td>\n",
       "      <td>-0.092730</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>-0.107473</td>\n",
       "      <td>0.098312</td>\n",
       "      <td>-0.081897</td>\n",
       "      <td>-0.284280</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.082985</td>\n",
       "      <td>-0.052379</td>\n",
       "      <td>-0.135277</td>\n",
       "      <td>0.122106</td>\n",
       "      <td>0.057177</td>\n",
       "      <td>-0.104051</td>\n",
       "      <td>-0.100320</td>\n",
       "      <td>0.024026</td>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>0.247197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0  0.054942  0.104693 -0.129839 -0.077362  0.066479 -0.007780   \n",
       "1           1 -0.014731  0.213879 -0.183907 -0.048958  0.112992  0.026316   \n",
       "2           2 -0.006671  0.217069 -0.125178 -0.073087  0.106584  0.016715   \n",
       "3           3  0.040802  0.123908 -0.106365 -0.109637  0.090377  0.020302   \n",
       "4           4 -0.018610  0.109863 -0.161603 -0.035534  0.143024 -0.053699   \n",
       "\n",
       "          6         7         8         9        10        11        12  \\\n",
       "0 -0.010820 -0.054412 -0.069162  2.085748 -0.060080  0.040206  0.078970   \n",
       "1  0.028143 -0.137892 -0.094396  2.170374 -0.155052  0.084018  0.093432   \n",
       "2  0.011751 -0.115226 -0.071845  2.141033 -0.135347  0.044908  0.057914   \n",
       "3 -0.004274 -0.007752 -0.103293  2.044155 -0.114577  0.053231 -0.004328   \n",
       "4 -0.026999 -0.152680 -0.012829  2.205195 -0.216011  0.017377  0.017984   \n",
       "\n",
       "         13        14        15        16        17        18        19  \\\n",
       "0 -0.092061 -0.113453 -0.105683 -0.018003  1.089996 -0.157291 -0.049872   \n",
       "1 -0.090761 -0.184210 -0.045192 -0.100942  0.870650 -0.183582  0.022500   \n",
       "2 -0.058195 -0.149449 -0.053666 -0.026909  0.822324 -0.158412 -0.001502   \n",
       "3 -0.138667 -0.142876 -0.056131 -0.052775  0.946257 -0.108830 -0.033371   \n",
       "4 -0.137153 -0.142915 -0.038127 -0.059814  1.205517 -0.104666  0.022861   \n",
       "\n",
       "         20        21        22        23        24        25        26  \\\n",
       "0 -0.038974 -0.044448 -0.051416  0.072666  0.006214 -0.023308 -0.103280   \n",
       "1 -0.002261 -0.022219 -0.042072 -0.042623 -0.055870  0.029976 -0.014145   \n",
       "2 -0.007176 -0.028327 -0.014629 -0.048186  0.021881  0.019163 -0.091954   \n",
       "3  0.017975  0.036760 -0.107292  0.045815 -0.012851 -0.082862 -0.054655   \n",
       "4 -0.049459 -0.069475 -0.073464  0.002518  0.027229 -0.035814 -0.045660   \n",
       "\n",
       "         27        28        29        30        31        32        33  \\\n",
       "0 -0.000791 -0.035179 -0.041778 -0.010981  0.052993 -0.096132  0.057582   \n",
       "1 -0.076327  0.112394 -0.168088 -0.052931  0.045638 -0.100293  0.127503   \n",
       "2 -0.019096  0.069852 -0.100524 -0.051520  0.076176 -0.067022  0.044913   \n",
       "3  0.042623 -0.008409 -0.040737 -0.051928  0.110341 -0.107192  0.056082   \n",
       "4  0.016310  0.109001 -0.021372 -0.066788  0.066605 -0.080589  0.068447   \n",
       "\n",
       "         34        35        36        37        38        39        40  \\\n",
       "0  0.072257 -0.147485 -0.058766 -0.006197 -0.030359 -0.055719  0.029750   \n",
       "1  0.101659 -0.109052 -0.011360  0.030617 -0.090257 -0.035377 -0.006508   \n",
       "2  0.044163 -0.114472 -0.039097  0.025448 -0.076740 -0.006505 -0.006603   \n",
       "3  0.109184 -0.118366 -0.025728 -0.035033 -0.067925  0.029779 -0.050414   \n",
       "4  0.259053 -0.222512  0.062160 -0.022953 -0.111141 -0.081637  0.030493   \n",
       "\n",
       "         41        42        43        44        45        46        47  \\\n",
       "0  0.133593  0.007728 -0.103093  0.096608  0.020460 -0.124611 -0.101497   \n",
       "1 -0.010191 -0.032645 -0.120629  0.083803  0.067324 -0.196460 -0.060567   \n",
       "2  0.046618  0.017113 -0.098088  0.080918  0.063744 -0.212404 -0.022673   \n",
       "3  0.064762  0.012224 -0.119392  0.147026  0.033925 -0.105274 -0.068832   \n",
       "4  0.199560 -0.042170 -0.110937  0.099936  0.065180 -0.122961 -0.050939   \n",
       "\n",
       "         48  ...       252       253       254       255       256       257  \\\n",
       "0  0.044355  ...  0.009718 -0.022423  0.052422  0.258125  0.172970  0.005742   \n",
       "1  0.115225  ... -0.112212 -0.036007  0.033733  0.198936  0.076721 -0.052154   \n",
       "2  0.051141  ... -0.064818 -0.047286  0.058058  0.155161  0.062122 -0.060407   \n",
       "3  0.058932  ... -0.029999 -0.057206  0.086667  0.267056  0.201220 -0.019701   \n",
       "4  0.069089  ... -0.077576  0.034792  0.051210  0.306868  0.168020 -0.054156   \n",
       "\n",
       "        258       259       260       261       262       263       264  \\\n",
       "0 -0.068247  0.043332  0.175139  0.131109 -0.030145  0.030684  0.077186   \n",
       "1 -0.087582  0.107428  0.068410  0.107855 -0.000844  0.111956  0.051873   \n",
       "2 -0.057822  0.058438  0.152636  0.142695  0.001692  0.106070  0.059557   \n",
       "3 -0.091312  0.085022  0.129807  0.108902 -0.002623  0.118156  0.087849   \n",
       "4 -0.084295  0.114701  0.031023  0.171025  0.012577  0.077978  0.072445   \n",
       "\n",
       "        265       266       267       268       269       270       271  \\\n",
       "0 -0.009650 -0.001977 -0.019755  0.067259 -0.036809 -0.027976 -0.018284   \n",
       "1  0.002043 -0.014444  0.032021  0.122599  0.021301 -0.031341 -0.088206   \n",
       "2 -0.014198 -0.027812 -0.003158  0.136810  0.004483  0.043852 -0.073878   \n",
       "3 -0.070147 -0.043028 -0.082752 -0.061191 -0.065372  0.000285 -0.036547   \n",
       "4 -0.051332 -0.000330 -0.014550  0.057516 -0.060990  0.019451 -0.079645   \n",
       "\n",
       "        272       273       274       275       276       277       278  \\\n",
       "0 -0.074472 -0.165668 -0.128944 -0.001251  0.025481  0.116750 -0.038316   \n",
       "1 -0.100880 -0.094794 -0.113471  0.018583 -0.000031  0.077720  0.028152   \n",
       "2 -0.047607 -0.103589 -0.124074  0.015846  0.018385  0.057015 -0.003170   \n",
       "3 -0.018116 -0.131600 -0.127061  0.053523  0.026109  0.114862 -0.014734   \n",
       "4 -0.029432 -0.204868 -0.190533 -0.024177  0.002055  0.121633  0.014271   \n",
       "\n",
       "        279       280       281       282       283       284       285  \\\n",
       "0  0.141899  0.243866  0.032853 -0.060356 -0.101602 -0.050094 -0.060763   \n",
       "1  0.201130  0.212862  0.003240  0.066036 -0.129031 -0.026765 -0.049921   \n",
       "2  0.170545  0.187558  0.017306 -0.009738 -0.094571 -0.017686 -0.045320   \n",
       "3  0.143633  0.312614  0.014281  0.006405 -0.085322 -0.002954 -0.006855   \n",
       "4  0.166662  0.304888  0.092535 -0.067335 -0.114079 -0.040758 -0.092730   \n",
       "\n",
       "        286       287       288       289       290       291       292  \\\n",
       "0  0.114494 -0.052732  0.121442 -0.079718 -0.238603  0.031822  0.059434   \n",
       "1  0.120074 -0.079767  0.147799  0.018992 -0.105756  0.051830  0.006578   \n",
       "2  0.086171 -0.079157  0.120796 -0.057970 -0.116192  0.020743  0.012045   \n",
       "3  0.155161 -0.058557  0.134907 -0.142102 -0.201305  0.014579 -0.002538   \n",
       "4  0.182500 -0.107473  0.098312 -0.081897 -0.284280  0.039023  0.082985   \n",
       "\n",
       "        293       294       295       296       297       298       299  \\\n",
       "0 -0.093304 -0.134071  0.053603  0.038264 -0.028437 -0.022459  0.068514   \n",
       "1 -0.007093  0.065427 -0.029093 -0.000948  0.012834  0.015731  0.002224   \n",
       "2 -0.028818  0.044761 -0.033804  0.015294 -0.019791  0.020420  0.016437   \n",
       "3 -0.046069 -0.155320  0.064200  0.079673 -0.035276  0.001438  0.066118   \n",
       "4 -0.052379 -0.135277  0.122106  0.057177 -0.104051 -0.100320  0.024026   \n",
       "\n",
       "          id    target  \n",
       "0  c12129c31 -0.340259  \n",
       "1  85aa80a4c -0.315372  \n",
       "2  b69ac6792 -0.580118  \n",
       "3  dd1000b26 -1.054013  \n",
       "4  37c1b32fb  0.247197  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_training_data = pd.read_csv(embedding_file)\n",
    "print(emb_training_data.shape)\n",
    "display(emb_training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values\n",
      "Unnamed: 0    0\n",
      "207           0\n",
      "205           0\n",
      "204           0\n",
      "203           0\n",
      "             ..\n",
      "99            0\n",
      "98            0\n",
      "97            0\n",
      "96            0\n",
      "target        0\n",
      "Length: 303, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing Values\")\n",
    "print(emb_training_data.isnull().sum().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature_name = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_X = emb_training_data.iloc[:, 1:-2]  # exclude the target label\n",
    "emb_y = emb_training_data[target_feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_X = bow_training_data.iloc[:, 1:-2]  # exclude the target label\n",
    "bow_y = bow_training_data[target_feature_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the Linear Regressor model as baseline\n",
    "baseline = linear_model.Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the Gradient Boosting Regressor on the daibetic retinopathy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_gbr = GradientBoostingRegressor(num_base_estimators=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the Gradient Boosting Regressor model on bag of words\n",
    "bow_gbr_scores = sklearn.model_selection.cross_val_score(custom_gbr, \n",
    "                                                         bow_X, bow_y, \n",
    "                                                         cv=cvfolds, \n",
    "                                                         scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBRegressor on bow - RMSE: 1.809 (0.196)\n"
     ]
    }
   ],
   "source": [
    "bow_gbr_scores = abs(bow_gbr_scores)\n",
    "print('GBRegressor on bow - RMSE: %.3f (%.3f)' % (np.mean(bow_gbr_scores), np.std(bow_gbr_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_valid_RMSE_comparisons[\"bow GBR\"] = bow_gbr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the Gradient Boosting Regressor model on the sentence embedding\n",
    "emb_gbr_scores = sklearn.model_selection.cross_val_score(custom_gbr, \n",
    "                                                         emb_X, emb_y, \n",
    "                                                         cv=cvfolds, \n",
    "                                                         scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBRegressor on emb - RMSE: 2.519 (0.188)\n"
     ]
    }
   ],
   "source": [
    "emb_gbr_scores = abs(emb_gbr_scores)\n",
    "print('GBRegressor on emb - RMSE: %.3f (%.3f)' % (np.mean(emb_gbr_scores), np.std(emb_gbr_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_valid_RMSE_comparisons[\"emb GBR\"] = emb_gbr_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn Model Performance\n",
    "Evaluate the performance of the a scitkit-learn classifier on the daibetic retinopathy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the baseline model on bag of words \n",
    "bow_lr_scores = sklearn.model_selection.cross_val_score(baseline, \n",
    "                                                        bow_X, bow_y, \n",
    "                                                        cv=cvfolds,\n",
    "                                                        scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR on bow - RMSE: 1.269 (0.110)\n"
     ]
    }
   ],
   "source": [
    "bow_lr_scores = abs(bow_lr_scores)\n",
    "print('LR on bow - RMSE: %.3f (%.3f)' % (np.mean(bow_lr_scores), np.std(bow_lr_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_valid_RMSE_comparisons[\"bow LR\"] = bow_lr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the baseline model on the sentence embedding\n",
    "emb_lr_scores = sklearn.model_selection.cross_val_score(baseline, \n",
    "                                                        emb_X, emb_y, \n",
    "                                                        cv=cvfolds,\n",
    "                                                        scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR on emb - RMSE: 0.658 (0.043)\n"
     ]
    }
   ],
   "source": [
    "emb_lr_scores = abs(emb_lr_scores)\n",
    "print('LR on emb - RMSE: %.3f (%.3f)' % (np.mean(emb_lr_scores), np.std(emb_lr_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_valid_RMSE_comparisons[\"emb LR\"] = emb_lr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bow GBR': array([1.98035442, 1.74065043, 1.97053596, 2.01026861, 1.7050703 ,\n",
       "        1.4336976 , 1.51837127, 2.00083402, 1.81927491, 1.91329422]),\n",
       " 'emb GBR': array([2.4285899 , 2.91262041, 2.62344132, 2.76556642, 2.47686792,\n",
       "        2.27430404, 2.32861419, 2.40971533, 2.53560754, 2.4352243 ]),\n",
       " 'bow LR': array([1.18956767, 1.45965528, 1.42917966, 1.39490569, 1.1646576 ,\n",
       "        1.27190421, 1.20866113, 1.20286103, 1.13738861, 1.22946898]),\n",
       " 'emb LR': array([0.60412894, 0.67672182, 0.67283618, 0.74049943, 0.6138729 ,\n",
       "        0.65476527, 0.66976956, 0.68931168, 0.58531628, 0.67472118])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_valid_RMSE_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the Root Mean Square Error (RMSE) by plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_df = pd.DataFrame.from_dict(model_valid_RMSE_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making a plot\n",
    "bp = plot.boxplot(RMSE_df) \n",
    "\n",
    "plot.ylabel('RMSE')\n",
    "plot.xticks([1, 2, 3, 4],\n",
    "           RMSE_df.columns,\n",
    "           rotation=45)\n",
    "# display the plot\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above boxplot indicates that the smallest Root Mean Square Error (RMSE) was achieved by training the **Linear Regressor (LR) model** on the sentence embedding (emb) representation. It means that Linear Regressor outperforms our Gradient Boosting Regressor on the CommonLit Readability Prize dataset without tuning hyparameters. The Gradient Boosting Regressor (GBR) achieved **smaller RMSE on the bag of words representation** than the sentence embedding representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 & 4: Add Subsampling and Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implement the GradientBoostingRegressor class adding subsampling and a learning rate as options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and RegressorMixin classes\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"GradientBoostingRegressor only works for continuous descriptive features and continuous target features.  \n",
    "        - Training: Train the new model to add to the ensemble by training the model to predict the errors (in regression terms the residuals) of the old model.\n",
    "        - Prediction: When a new prediction needs to be made compare the descriptive feature values of the new query instance to each tree and return the target feature that is close to the query case. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_base_estimators numeric, required (default = 1) \n",
    "        The number of estimators to include in the model.\n",
    "    subsampling_of_rows boolean, optional (default = False)\n",
    "        Used to train each base estimator to be turned on or off.\n",
    "    learning_rate numeric, optional (default = 0)\n",
    "        Reduces the influence of models trained later in the ensemble building process.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    trees_: dict\n",
    "        A dictionary of the DecisionTree on all descriptive features in each epoch.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> reg = GradientBoostingRegressor()\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(reg, iris.data, iris.target, cv=10)\n",
    "    \"\"\"\n",
    "    # Constructor for the regressor object\n",
    "    def __init__(self, \n",
    "                 num_base_estimators = 5,\n",
    "                 subsampling_of_rows = False, \n",
    "                 subsampling_ratio = 0.5, # percentage of rows selected per tree building\n",
    "                 learning_rate = 1e-3, # default learning rate\n",
    "                 tree_depth=5,  # the depth of decision tree\n",
    "                 min_samples_leaf=1, # the mini number of samples required to be at a leaf node\n",
    "                 random_state=None\n",
    "                ):\n",
    "        self.num_base_estimators = num_base_estimators\n",
    "        self.subsampling_of_rows = subsampling_of_rows\n",
    "        self.subsampling_ratio = subsampling_ratio\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tree_depth = tree_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.random_state = random_state\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build an educated guess regressor from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csc_matrix``.\n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"       \n",
    "            \n",
    "        # change instance from array to 2D matrix\n",
    "        X = np.array(X)      \n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        # Set up the random number generator to be used to generate \n",
    "        # predictions - this follows reccommended scikitlearn pattern\n",
    "        # https://scikit-learn.org/stable/developers/develop.html#coding-guidelines\n",
    "        self.random_state_ = check_random_state(self.random_state)    \n",
    "        \n",
    "        if self.subsampling_of_rows:\n",
    "            # subsamplling: 50% of input data X\n",
    "            rand_size = int(X.shape[0] * self.subsampling_ratio)\n",
    "            # subsample the input X by replacement \n",
    "            rand_idx = np.random.choice(X.shape[0], rand_size, replace=True)\n",
    "            # select the subset of X and y respectively\n",
    "            X = X[rand_idx,:]\n",
    "            y = y[rand_idx]     \n",
    "        \n",
    "        # store all generated trees\n",
    "        self.trees_ = []\n",
    "        \n",
    "        self.m0 = y.mean()\n",
    "        M = self.m0        \n",
    "        \n",
    "        for i in range(self.num_base_estimators):\n",
    "            # compute residuals\n",
    "            res = y - self.m0\n",
    "            # fit DecisionTree on X and residuals\n",
    "            tree = sklearn.tree.DecisionTreeRegressor(max_depth=self.tree_depth,\n",
    "                                                      min_samples_leaf=self.min_samples_leaf,\n",
    "                                                      random_state=self.random_state)\n",
    "            tree.fit(X, res)\n",
    "            delta = tree.predict(X)\n",
    "            # add delta to the model M \n",
    "            M = M + self.learning_rate * delta\n",
    "            # append the tree\n",
    "            self.trees_.append(tree)\n",
    "            \n",
    "        # Return the regressor\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict continues value of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "            The input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
    "            to a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        M : array of shape = [n_samples, ].\n",
    "            The predicted values of the input samples. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check is fit had been called by confirming that the trees_ dictionary has been set up\n",
    "        check_is_fitted(self, ['trees_'])\n",
    "        \n",
    "        # change instance from array to 2D matrix\n",
    "        X = np.array(X)             \n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        M = self.m0\n",
    "        \n",
    "        for i in range(self.num_base_estimators):\n",
    "            # use trees to predict target feature\n",
    "            M = M + self.learning_rate * self.trees_[i].predict(X)\n",
    "            \n",
    "        return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hyper-parameter tuning and evaluate models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid ={\n",
    "    'num_base_estimators': [3, 15, 30],\n",
    "    'subsampling_of_rows': [True, False], \n",
    "    'learning_rate': [0.003, 0.015, 0.03]\n",
    "}\n",
    "cvfolds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search\n",
    "emb_tuned_gbr = sklearn.model_selection.GridSearchCV(GradientBoostingRegressor(),\n",
    "                                                    param_grid, cv = cvfolds, \n",
    "                                                    scoring = scoring, verbose = 2,\n",
    "                                                    return_train_score = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do hyperparameter tuning on the sentence embedding representation dataset with 5 folds cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.003, 0.015, 0.03],\n",
       "                         'num_base_estimators': [3, 15, 30],\n",
       "                         'subsampling_of_rows': [True, False]},\n",
       "             return_train_score=True, scoring='neg_root_mean_squared_error',\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_tuned_gbr.fit(emb_X, emb_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'learning_rate': 0.03, 'num_base_estimators': 30, 'subsampling_of_rows': False}\n",
      "0.8776150289339701\n"
     ]
    }
   ],
   "source": [
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(emb_tuned_gbr.best_params_)\n",
    "model_tuned_params_list[\"Tuned GBR emb\"] = emb_tuned_gbr.best_params_\n",
    "print(abs(emb_tuned_gbr.best_score_))\n",
    "model_valid_RMSE_comparisons[\"Tuned GBR emb\"] = abs(emb_tuned_gbr.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_scores = sorted(abs(emb_tuned_gbr.cv_results_['mean_test_score']), reverse = True)\n",
    "train_scores = sorted(abs(emb_tuned_gbr.cv_results_['mean_train_score']), reverse = True)\n",
    "\n",
    "plt.plot(test_scores, label='test')\n",
    "plt.plot(train_scores, label='train')\n",
    "plt.title('GBR GridSearch Validation Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do hyperparameter tuning on the bag of words representation dataset with 5 folds cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search\n",
    "bow_tuned_gbr = sklearn.model_selection.GridSearchCV(GradientBoostingRegressor(),\n",
    "                                                    param_grid, cv = cvfolds, \n",
    "                                                    scoring = scoring, verbose = 2,\n",
    "                                                    return_train_score = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   4.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   7.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   3.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   8.0s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   3.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   7.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.4s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   2.2s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.2s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.2s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.003, 0.015, 0.03],\n",
       "                         'num_base_estimators': [3, 15, 30],\n",
       "                         'subsampling_of_rows': [True, False]},\n",
       "             return_train_score=True, scoring='neg_root_mean_squared_error',\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tuned_gbr.fit(bow_X, bow_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'learning_rate': 0.015, 'num_base_estimators': 30, 'subsampling_of_rows': False}\n",
      "1.0135498141658146\n"
     ]
    }
   ],
   "source": [
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(bow_tuned_gbr.best_params_)\n",
    "model_tuned_params_list[\"Tuned GBR bow\"] = bow_tuned_gbr.best_params_\n",
    "print(abs(bow_tuned_gbr.best_score_))\n",
    "model_valid_RMSE_comparisons[\"Tuned GBR bow\"] = abs(bow_tuned_gbr.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1yklEQVR4nO3deXxU5fX48c/JvieEhC0BEwQVRECMCAouiApoRa3Ffalaytdqta2t2n7V9vv9tfXbxapVoG5V6lY3FBUrakXqwr6vgiwSQAggBAIh2/n98dzAMEySASazJOf9Yl5z5z53OXMznOcuz32uqCrGGGNal7hIB2CMMSb8LPkbY0wrZMnfGGNaIUv+xhjTClnyN8aYVsiSvzHGtEKW/E3IichuEenaQNmNIvJpuGNqTDhjEhEVkW7e8HgRuS+YaY9gPdeIyJQjjdO0fJb8Y4SIXCkiM0SkQkS2eMO3ioh45c+KSJWXeHeJyBwROctn/htFpNYrLxeRBSJyURPrzBSRh0Rkrbfer0XkNRHp39h8qpqhqquD/F4jRWS+F9NWEflIRIqCmTcSROR9EfmfAONHisg3IpIQ7LJUdYyq/m8IYiryKor961bVF1T1/KNddgPryxKRh73fw24RWeV9zmuO9ZnmYck/BojIz4BHgD8CHYD2wBjgDCDJZ9I/qGoGkA2MA94QkXif8i+88hxgLPCyiOQ0sM5k4N/AScBFQBbQA3gZGNHAPEEnPm/6bsAE4GdezMVeXHWHs5wg1xXf9FRBeRa4rr7S9XEd8IKq1oRoPVFJRJKAj4ATgWG438XpwDag0Z2CBpZ3WL8ZE0Kqaq8ofuGSYgXw3Samexb4fz6f0wAFOnmfbwQ+DVB+agPLuwXYBKQ3sV4FfgSsBNb4jOvmDbcFJgHlwEzgf+vjAC4H5jey7DjgHuArXHJ5Bcj1KX8V+AbYCUwDTvTbHuOAyd72Gwp0Bt4AyrzlPea7bYA/Ad8Ca4DhDcSU6q3vTJ9xbYBKoA8uAX4B7PC232NAkt/26uYTo+/f7OfePBuBm/ymvRCY523H9cCvfeb72pt2t/caGODvfTowy4t9FnC6T9lU7+/yGbALmALkNfK72AxkNPGb6BbotwmcDZQCd3t/u38Ay4CLfKZPALYC/bzPA4DPvW26ADg70v8vW8LL9vyj30AgGXgr2Bm8vdzrcUlscwPl3weqgXUNLGYo8L6qVgSxykuA04CeAcoexyXGjriEdpNP2VzgBBH5i4icIyIZfvP+2Fv2WUAnXGJ+3Kf8PaA70M5b1gt+818N/BbIxCXkd3DftwgowB3F1DsNWAHkAX8Ang6wd4+q7sVVQtf7jB4FLFfVBUAt8BNvOQOBc4FbD9kqfkRkGHAXcJ73nYb6TVLhrTMHVxH8l4hc4pWd6b3nqDvl9oXfsnOBd4FHcZXxQ8C7ItLWZ7Krcb+JdrijybsaCHUo8C9V3d3Ud2pEByAXOAYYDbwEXOVTfgGwVVXnikiBF/v/8+a5C3hdRPKPYv0GbM8/2l/AtcA3fuPq94L24u2B4vauKr3xld7rGp95bgRqvPJqb95Rjaz3Q+BBn899vXnLgRU+4xUY4jevAt2AeG9dJ/iU/Y6D90gH4JJpmRfzs3h7lbg9wnN9pu3oLS8hQLw53nqzfbbHBJ/ygd46As17I7DK53P9UVGHBrbNINwedKr3+TPgJw1Meycw0X/b+MRYv0f8jN/2Pg6/PWi/5T4M/MUbLvKmTfD7TvVHWNcBM/3m/wK40RueCvy3T9mtuAQfaL0f+MbZwDRN7flXASk+5d1wRxxp3ucXgPu94buBf/gt/33ghkj+v2wJL9vzj37bgDy/i3mnq2qOV+b7N/yTNz4VKAH+KCLDfcqne+VtcKdiBjex3o4+65zvzXsZ7kjE1/oGlpGPO4T3LT/oSENVp6vqKFXN9+I5E/iVV3wMMFFEdojIDlxlUAu0F5F4EXlQRL4SkXJgrTeP70VH3/V2BtZpw+fkv/GJaY836H8kUl/+Ka4iGem1ajoVeBFARI4TkXe8i7/luMoumAuhnWhkO4nIaSLysYiUichO3DWfYC+wdvJfnve5wOfzNz7De2jgu+P3uzhCZapaWf9BVVfh/rbfEZE04GK87Yn7DXyv/jfg/Q4GhSCGVs+Sf/T7AtgHjAx2BnUW4/ZILwxQvhu3d3ediJzcwGI+As4XkfRgVtnA+DLc0UZnn3FdGol7Fu6cfC9v1Hrcufccn1eKqm7AnaYYiTsNkY3b+wXwPVXjG9d6oEsILzBOwJ2GuQ6Yoqr1p9fGAcuB7qqaBfzSL6aGbKLx7fQirsLurKrZwHif5TbVNe9GXBL11QXYEERc/j4ELmjid7EHd/RUr4NfeaB460/9jASWehUCuL/bP/x+A+mq+uARxG58WPKPcqq6A/gNMFZELheRDBGJE5G+QIP/AUXkBNwe0pIGlrsNeAq4v4FFTMAlpIki0svb007BHVEEG3stLpn/WkTSRKQncINPjINE5Aci0s4n5ouB6d4k44HfisgxXnm+iNRXgpm4SnEbLtH8rolwZnrf50ERSReRFBE5I9jvEsAEXMXzA+A5n/GZuFNju73v819BLu8V4EYR6ent/T7gV54JbFfVSq+p7dU+ZWW4FlIB763AXfQ+TkSuFpEEEbkCd33mnSBj8/UPXEJ+XURO8H6LbUXklyJS3wpsPnC195sZhrtm05SXgfNx2+tFn/HP444ILqj/DYrI2SJSeASxGx+W/GOAqv4B+CnwC2AL7iLu33DnQz/3mfQXXrvrClyLjb970zXkYWCEiPQOsM5K4BxgKe6CWznuguipuAucwboNdwrhG9y537/7lO3AJftFIrIb+BcwEXfBFVzz1knAFBHZhasUTvPKJuBOXWzwYpxOI7yK6Du488tf41qcXHEY38N/eWtx2z7di7HeXbjEvAt4EvhnkMt7D/f3+Dewynv3dSvwP952uB9XWdTPuwd3Yfsz79TIAL9lb8M11/0ZrrL8Ba51zdZgYvNb1j5cpbccd/6/vhVXHjDDm+wO3LbeAVwDvBnEcjfhjnJPx2ebqep63NHAL3GV3HpcqyjLXUdJVO1hLsYY09pY7WmMMa2QJX9jjGmFLPkbY0wrZMnfGGNaoSbbPIvIM7iWAltUtVeAcsG1yhiBa997o6rO9cpycM0Je+Ha9t6kfreeB5KXl6dFRUXBfwtjjGnl5syZs9W7WTIowdzw8iyuc6oJDZQPx/VF0h3XDG8cB5rjPYK7TfxyrzfAtMCLOFhRURGzZ88OZlJjjDGAiDTUT1dATSZ/VZ3WRP/qI3F9qCgwXURyRKQjriOqM3F9jKCqVbg+PYwxxkRYKM75F3BwnySl3riuuJsy/i4i80TkqSC7CjDGGNPMQpH8A/Vborijin7AOFU9GXckcE+DCxEZLSKzRWR2WVlZCMIyxhjTkFB0clXKwR1SFeI6klKgVFXrb/l+jUaSv6o+ATwBUFJSYrcdG2MOS3V1NaWlpVRWVjY9cQxLSUmhsLCQxMTEo1pOKJL/JOA2EXkZd6F3p9dPByKyXkSOV9UVuIdaLA3B+owx5hClpaVkZmZSVFREgOfwtAiqyrZt2ygtLaW4uPiolhVMU8+XcA9gyBORUlxvg4leIONxPQaOwHVGtQf3NKB6twMveC19VvuVGWNMyFRWVrboxA8gIrRt25ZQnBoPprXPVU2U1z/DNVDZfA6jC2BjjDkaLTnx1wvVdwzVgy2iwqMfrSQxPo7s1MSAr8yUBOLiWv6PwxhjmtJikr+qMv6Tr9hTVdvgNCKQmZxAdtqBCiErxWfYe89JS6QgJ5XivHRy0pLC+C2MMbFqx44dvPjii9x6662HPe/DDz/M6NGjSUsL6j7YkGgxyV9EWPKbC9hbXUv53hp27q0O+Cr3+7y5fPf+4aqaukOWm5OWSFHbdIrz0ilqm05RXpobzksnK+XorrYbY1qOHTt2MHbs2CNO/tdee60l/yMlIqQlJZCWlECH7JTDnr+yupade6v5dk8V67fvZe3WCtZsq2Dt1gpmrN7GxHkHP/K0bXoSRV6lUJyX5jOcTnpyi9q0xpgm3HPPPXz11Vf07duX8847j3bt2vHKK6+wb98+Lr30Un7zm99QUVHBqFGjKC0tpba2lvvuu4/NmzezceNGzjnnHPLy8vj444/DEq9lKB8pifGkJMbTPiuFEzpkHVJeWV3Lum17WLO1grVepbB6awX/WVnG63P3HTRtfmYyXfPSObdHOy7uU3BElZEx5sj85u0lLN1YHtJl9uyUxQPfObHB8gcffJDFixczf/58pkyZwmuvvcbMmTNRVS6++GKmTZtGWVkZnTp14t133wVg586dZGdn89BDD/Hxxx+Tl5cX0pgbY8n/MKQkxnN8h0yO75B5SFnFvhqvQtjD2m0VrNlawbJN5fxu8nJ+/95yzjg2j0tOLmBYrw5k2FGBMS3alClTmDJlCieffDIAu3fvZuXKlQwePJi77rqLu+++m4suuojBgwdHLEbLQiGSnpzAiZ2yObFT9kHjV5ft5s35G3lz3gbuenUB//3mIs7v2YFL+xUwuFseCfH2SAVjQq2xPfRwUFXuvfdefvjDHx5SNmfOHCZPnsy9997L+eefz/333x+BCC35N7uu+Rn89Lzj+MnQ7sz9+lvemLuBdxZuYtKCjeRlJPGdPp249OQCTirIbhVtlI1pqTIzM9m1axcAF1xwAffddx/XXHMNGRkZbNiwgcTERGpqasjNzeXaa68lIyODZ5999qB57bRPCyQinHJMLqcck8v93+nJ1BVlvDlvAy9M/5q/f7aWY/PTufTkAkb2LaBzbviu+BtjQqNt27acccYZ9OrVi+HDh3P11VczcOBAADIyMnj++edZtWoVP//5z4mLiyMxMZFx48YBMHr0aIYPH07Hjh3DdsFX3A260aWkpERby8Ncdu6pZvLiTUycu4GZa7cD0L8ol0v7FTCiV0ey06w5qTHBWLZsGT169Ih0GGER6LuKyBxVDbpHBdvzj7DstESu6t+Fq/p3Yf32Pbw1fwNvzNvAvW8s4oG3ljDkhHZc1KcjbYK42aypk0ZJCXF0b5dpFYoxxpJ/NOmcm8ZtQ7rzo3O6sWjDTibO28DbCzbyryXfhHQ9hW1S6dUpmxM7ZXFiQRa9OmXTLsuaohrTmljyj0IiQu/CHHoX5vCrET1YvLE84N3HvoI5fbenqpZl35SzZEM5SzbuPKhSyctIdpVBpyx6FbiKoUtuml2ENqaFsuQf5RLi4+jbOSdkyzvnhHb7h3dVVrNs0y4Wb9jJko2uQvh01VZq61xFkpmcQM9OWV4TVlcpHJufbs1TjWkBLPm3YpkpifQvzqV/ce7+cZXVtXy5eRdLNpbvrxRenLmOymp35JGcEMcJHbPoW5hNn8459OmcQ3HbdOst1ZgYY8nfHCQlMX7/Kad6NbV1rN5awZKNO1myoZyFG3by6pxSnvtiHQCZKQn0Kcyhr1cZ9OmcTbtMu4ZgTDSz5G+alBAfx3HtMzmufSaXurvVqa1TVm3ZzYL1O5i3fgcL1u9g3Cdf7T9l1Ck7Zf+RQZ/CHHoXZltnd6ZFO9IunUeMGMGLL75ITk5O8wTWAPvfaI5IfJzs7+do1KmdAdhbVcuSjTuZv34HC0p3smD9Dt5b7C4qxwl0b5dJn87Z+yuE7u0zSIqPs4vKpkVoqEvn2tpa4uPjG5xv8uTJzR1aQJb8TcikJsVTUpRLSdGBawjbK6pYsH6HVyHs4IOlm3lldulB88XHCQlxQmJ8HPFxQmK8eOPi9g/XlyXEx5EY5zcuTnD1h3uPE5D9wwLuHyLilblh8ZlOgIR4t8yk+DgSE+K8YTcu0RuXHB9HYsKBcUn1ZfFCkjdPUV66dd7XCvl26ZyYmEhGRgYdO3Zk/vz5LF26lEsuuYT169dTWVnJHXfcwejRowEoKipi9uzZ7N69m+HDhzNo0CA+//xzCgoKeOutt0hNTW2WeO0XappVbnoS55zQbn8rI1Vl/fa9zC/dwbqtFVTXKbV1ddTUKjV1Sk1tnRtXq1TX1VFbp9TUKtW1brh++upaZW91LTW1ddTUKaqgHGjyWqcHxtV5A/XldQqKV65unAI1dW491TVu+VW1jTevbUh6UjzfK+nM9QOPoWt+Rki2ozlM790D3ywK7TI7nATDH2yw2LdL56lTp3LhhReyePFiiouLAXjmmWfIzc1l7969nHrqqXz3u9+lbdu2By1j5cqVvPTSSzz55JOMGjWK119/nWuvvTa038PTZPIXkWeAi4AtqtorQLkAjwAjgD3Ajao616c8HpgNbFDVi0IVuIlNIkKXtml0aRv9/Rep6v4KoaqmjqpaVym4yuHA5yqfz5VVtXywbDMvzviaZz9fy1nH5XPjGUWc1T3fWkS1Mv3799+f+AEeffRRJk6cCMD69etZuXLlIcm/uLiYvn37AnDKKaewdu3aZosvmD3/Z4HHgAkNlA8Hunuv04Bx3nu9O4BlwKFPRzEmiom4U1CJ8XEczqOch5/UkXuH9+ClmV/z/PR1fP/vsyjOS+f6gcdw+SmFZNrjP5tfI3vo4ZKenr5/eOrUqXz44Yd88cUXpKWlcfbZZ1NZWXnIPMnJyfuH4+Pj2bt3b7PF1+TdOqo6DdjeyCQjgQnqTAdyRKQjgIgUAhcCT4UiWGNiRX5mMj8+tzuf3j2ER686mTZpifzm7aUM+N1H/HrSElaX7Y50iCbEfLt09rdz507atGlDWloay5cvZ/r06WGO7lChOOdfAKz3+VzqjdsEPAz8Ajj00Vd+RGQ0MBqgS5cuIQjLmMhLSojj4j6duLhPJxas38Fzn6+1U0ItlG+XzqmpqbRv335/2bBhwxg/fjy9e/fm+OOPZ8CAARGM1AmqS2cRKQLeaeCc/7vA71X1U+/zR7iE3xEYoaq3isjZwF3BnvNvTV06m9anbNe+/aeEtuzaZ6eEQsS6dD68Lp1D0UlLKdDZ53MhsBE4A7hYRNYCLwNDROT5EKzPmJjme0rokSv7kmOnhEwEhCL5TwKuF2cAsFNVN6nqvapaqKpFwJXAv1W1edosGRODkhLiGNm3gIm3nsFbPzqDC07swAsz1jHkz59wwzMzmbPu20iHaFqwJpO/iLwEfAEcLyKlInKziIwRkTHeJJOB1cAq4Eng8O5tNsbQp3MOD13Rl8/uGcJPhh7H0k3lXD7+cx54azG799VEOryYEY1PJgy1UH1He4yjMVFo974a/vT+Cp77Yi0ds1L47aUnHdQdtznUmjVryMzMpG3bti22yxBVZdu2bezateugewjg8M/5W/I3JorNWfctd7++kFVbdjOybyfuv6gnbTOSm56xFaqurqa0tDRg+/mWJCUlhcLCQhITD24cYMnfmBZmX00tYz/+irFTV5GZksj9F/VkZN9OLXbv1hyZSLT2McY0o+SEeH5y3nG8c/tguuSmcec/5/P9Z2exYUfz3f1pWj5L/sbEiOM7ZPL6f53O/Rf1ZMbq7Zz/0Cc89/la6uqi7+jdRD9L/sbEkPg44aZBxUz5yZn0O6YND0xawuXjP2fl5sDdChjTEEv+xsSgzrlpTLipP3/+Xh9Wb63gwkc/5ZEPV1JVc2TdUJvWx5K/MTFKRPjuKYV8+NOzOP/E9vzlwy/5zl8/Zd7XdnOYaZolf2NiXF5GMo9d3Y+nri9h595qLhv3Of/z9lL2VNnNYaZhlvyNaSGG9mzPBz89k2tO68Izn63h/L9MY9qXZZEOy0SpltXOf+M8iEuA+GRISIKEFIhPgoRkNxzX8EOUjWlJZq7Zzj2vL2T11gryMpJpl5lMuyzvPTNl/3B+Zsr+suQE+/8Ryw63nX/Leobv30dA9Z6GyyXeqwiSvQoi0HASJKbBsedAn6sg2Z7BamJP/+JcJt8xmBdmfM2qLbvYUr6PLbv2sWxTOWW79hGodWhOWuKByiEzmfysA8PtMpNpk55EVkoi2amJpCTG2U1mMa5l7fmv/ACq90JtFdTsg9p97r1mnzeu0m+4ypvG+1w/397tsG0VJGdB36vh1B9AXrfQf1FjIqC2TtlWsY8t5fso27WPLbsq91cOW3ZVunevrKGH2CfFx5GVmkBWqqsMslMT91cM2amJZKUm+AwfKMtNTyI9uWXtc0aL1r3n3/280C2rdDbM+BvMehpmjIduQ6H/D917nF0qMbErPk68PfqURqdTVXbsqd5fKezcW73/Vb63xnuvpryymu0VVazZWrF/XGP3nZ17QjtuHlzMwK4ttwO2WNCy9vybw67NMPc5Vwns/gbaFEP/H0DfayA1J9LRGRN1VJXd+2oOqiR2epXEmq0V/HPWerZXVHFipyxuGVzMhSd1IinBdqiOlnXs1lxqq2HZJJjxBKyf7q4L9L4C+o+G9j0jHZ0xMaOyupaJ8zbw1H9W81VZBR2yUrjh9CKu7t+F7DR7jOWRsuQfDpsWwMwnYNFr7lpB0WBXCRw/AuJb1pk0Y5pLXZ3yycoynv7PGj5dtZXUxHhGlRRy06BijmmbHunwYo4l/3Das/3AKaGd6yG7M5TcBP1ugPS2kY7OmJixdGM5T3+6hkkLNlBTp5zfsz23DO5KyTFt7LpAkCz5R0JdLax4D2b+DdZMc01HT7ocTr0FOvS2owFjgrSlvJIJX6zj+Rnr2LGnmj6F2dwyuCvDe3UgId6uCzTGkn+kbVkGM5+EBS9DdQUgkJ4PGe0hsz1kdDj0PaMdZHaAxNRIR29MVNhTVcPrczfwzKdrWLO1goKcVG48vYgr+ncmK8WuCwQS8uQvIs8AFwFbVLVXgHIBHgFGAHuAG1V1roh0BiYAHYA64AlVfSSYoGI6+dfbuwOWvwM7voZd38DuzQfed28BrT10nuRsr0Jo7yqDDJ/h9DxXiaTnQ1pbiLf/AKblq6tTPlq+haf+s5oZa7aTkZzAFad25sbTi+icmxbp8KJKcyT/M4HdwIQGkv8I4HZc8j8NeERVTxORjkBHryLIBOYAl6jq0qaCahHJvzF1tbBnm1cZbHFNSP0riPr3mgaeR5qSc6AySM87tHLYX5YPqW3s3gQT8xaV7uSpT1fz7sJNKHB5v0J+PLQ7BTl2xAzNdNpHRIqAdxpI/n8DpqrqS97nFcDZqrrJb7q3gMdU9YOm1tfik3+wVKFyp6sgKspgz1b3XrHVe5UdeN+z1V2AJsDfU+JchZDaxnVxcbRSsqHf9e66RoI9TNyE18Yde3li2mpenPE1AFef1oVbzzm2yZvWWrpIJP93gAdV9VPv80fA3ao622/+aUAvVS1vYB2jgdEAXbp0OWXdunXBfgdTr7bGdU2xv2Ioc0cY9cN7v3UVytHa+iWULYf0du6idslNkJF/9Ms15jBs2LGXv360klfnlJIYL9x4ejE/PLMrbdKTIh1aREQi+b8L/N4v+f9CVed4nzOAT4DfquobwQRle/5RThVWT4UvHodVH7jWTb1HwYBb7YY3E3Zrtlbw8IdfMmnBRjKSErh5cDE3Dyoms5VdGI6q0z4ikgi8A7yvqg8FG5Ql/xhStgKmj3Otm2r2QtdzYOCP4Nhz7TqDCasV3+zioQ9W8P6SzbRJS2TMWcdy/cAiUpNaR1fVkUj+FwK3ceCC76Oq2t9rBfQcsF1V7ww2ILDkH5P2bIfZz7hmrru/gbzjYMB/Qe8rIclaZZjwWVi6gz9N+ZJpX5bRLjOZ24Z048pTu7T4/oOao7XPS8DZQB6wGXgASARQ1fFekn8MGIZr6vl9VZ0tIoOA/wCLcE09AX6pqpObCsqSfwyrqYIlE2H6464bjNRcKPm+6xY7q2OkozOtyIzV2/jzlC+ZuXY7BTmp3DG0O5edXNBibxazm7xMdFCFdZ/D9LGw/F33hLVe34WBt0LHPpGOzrQSqsq0lVv50/srWLRhJ13z0/nJ0OO48KSOxMW1rG4jLPmb6LN9tXs2wrznoWo3HDPIVQLHDbNHa5qwUFXeX7KZhz5YwZebd9OjYxY/O+84zu3RrsX0HWTJ30SvvTtg3j9cRbBzvbsBLe94yC2GtsdCblf3alNsj880zaK2Tnl7wUb+8uGXrNu2h+PbZ9I5N23/k8eyUhL3P50sKyVh/5PI6t/Tk+KjtrKw5G+iX20NLH8bvnzfHRVsX+3uQ/CV0f5AZZBb7DPc1d1kZsxRqK6t47U5pUyav5Ed9U8k21vNrn01jc4XHydkpST4VBDukZUZyQnExwkiQrwI8XFCnAjxcRAX5z9OiJPA49OS4vleSecj+k6W/E1sqiyHb9ccqAy2r4bt3uddmw6eNq3twZVB7rHuyKFtN0jJikz8pkWorVN2VbonkJVXVh/0qMr6J5MdPN49qaxiXw21dUqdKrV16g2z//OB98bXn5+ZzKxfDT2i2Fv3M3xN7ErJcheCA10MrqqAb9f6VQyr3QXlha9wUJcWGe2hbfcDlUFed/fepsg6wzNNio8TctKSyElrnruEVV0FcFBFoUqdV2GEc1fckr+Jfknp0P5E9/JXXemOGLatcq+t3vvyd1zXFvUk3lUA9ZVB22O9SqKb6zU1Ss/jmpbFnRZylUykWfI3sS0xBdr1cC9/e7bDtq8OVAzbVrrPq6ce3FtqUoarDNLzgRD8p+xzpev0zpgoZsnftFxpue7V+dSDx9fVQfmGA5XB1pVueM/2o19nxVaYOMYdYdj9DCaKWfI3rU9cHOR0dq9jh4R22Xu2w9iB8MZoGD3Vns5molbLvM/ZmEhJy4VLxrourz/8daSjMaZBlvyNCbVu58JpY2DGeFj1UaSjMSYgS/7GNIehv4b8E+DNW0NzLcGYELPkb0xzSEyFy550zU3f/nFonqBmTAhZ8jemuXTsDUP+G5a9DfNfjHQ0xhzEkr8xzen0210vpu/9wnVXYUyUsORvTHOKi4dLx4HEufb/tY13HGZMuFjyN6a55XSBC/8M66fDZ3+JdDTGAJb8jQmPk74HJ14GUx+EDXMjHY0xlvyNCQsRuOgh1+voG6Ohak+kIzKtXJPJX0SeEZEtIrK4gXIRkUdFZJWILBSRfj5lw0RkhVd2TygDNybmpLaBS8a5foQ+uC/S0ZhWLpg9/2eBYY2UDwe6e6/RwDgAEYkHHvfKewJXiUjPownWmJjX9SwYeBvMegq+nBLpaEwr1mTyV9VpQGO3KI4EJqgzHcgRkY5Af2CVqq5W1SrgZW9aY1q3IfdBuxPhrR+5XkCNiYBQnPMvANb7fC71xjU03pjWLTEFvvskVO6ASXb3r4mMUCT/QE+/0EbGB16IyGgRmS0is8vKyhqazJiWof2JcO4DsOJdmDsh0tGYVigUyb8U8H3cfCGwsZHxAanqE6paoqol+fn5IQjLmCg34FYoPhP+da97qIwxYRSK5D8JuN5r9TMA2Kmqm4BZQHcRKRaRJOBKb1pjDLiHylwyHuITXPNPu/vXhFEwTT1fAr4AjheRUhG5WUTGiMgYb5LJwGpgFfAkcCuAqtYAtwHvA8uAV1R1STN8B2NiV3YBXPQX2DAb/vOnSEdjWpEmH+Ooqlc1Ua7Ajxoom4yrHIwxDen1XfjyffjkD9BtKBSWRDoi0wrYHb7GRIMRf4SsTvDGD2Df7khHY1oBS/7GRIOUbLh0vOv2+f1fRjoa0wpY8jcmWhQNgjN+DHOfg+V2ttQ0L0v+xkSTc34FHU6CSbfBrs2Rjsa0YJb8jYkmCclw2VNQVQFPDoGlb9kdwKZZWPI3Jtq0OwGunwSpOfDK9fCPS6Hsy0hHZVoYS/7GRKMup8HoT2D4H9zDX8YNhCn3wb5dkY7MtBCW/I2JVvEJcNoP4fY50OdK+PxReOxUWPSanQoyR82SvzHRLiMfRj4ON38IGe3g9Zvhue/A5qWRjszEMEv+xsSKzqfCDz523UFsXgzjB7lO4Sp3RjoyE4Ms+RsTS+LioeQmuH0u9LsOpo+Dv5bA/JfsVJA5LJb8jYlFabnwnUfgB/+GnC7w5hh4ZhhsWhjpyEyMsORvTCwr6Ac3fwAXP+YeDP/EWfDuXbD320hHZqKcJX9jYl1cnDsFdPscOPUWmP00/PUU94SwurpIR2eilCV/Y1qK1Daud9DRn0Db7jDpdnh6qD0lzARkyd+YlqZjb7jpX3Dp31zif/UGqKmKdFQmyljyN6YlEnE3hl0yFr5ZBJ/8X6QjMlHGkr8xLdkJF0Lfa+DTh6B0dqSjMVHEkr8xLd2w30NWAUz8IVTtiXQ0JkpY8jempUvJdt1DbFsFH/460tGYKBFU8heRYSKyQkRWicg9AcrbiMhEEVkoIjNFpJdP2U9EZImILBaRl0QkJZRfwBgThK5nwWljYObfYPXUSEdjokCTyV9E4oHHgeFAT+AqEenpN9kvgfmq2hu4HnjEm7cA+DFQoqq9gHjgytCFb4wJ2rkPQNtu8OaPrD8gE9Sef39glaquVtUq4GVgpN80PYGPAFR1OVAkIu29sgQgVUQSgDRgY0giN8YcnqQ01/xz10bXIZxp1YJJ/gXAep/Ppd44XwuAywBEpD9wDFCoqhuAPwFfA5uAnao6JdBKRGS0iMwWkdllZWWH9y2MMcEpLIFBP4X5L8DydyMdjYmgYJK/BBjn333gg0AbEZkP3A7MA2pEpA3uKKEY6ASki8i1gVaiqk+oaomqluTn5wcbvzHmcJ11t3tI/Nt3QMXWSEdjIiSY5F8KdPb5XIjfqRtVLVfV76tqX9w5/3xgDTAUWKOqZapaDbwBnB6KwI0xRyghCS59wp33f/sO6wq6lQom+c8CuotIsYgk4S7YTvKdQERyvDKAW4BpqlqOO90zQETSRESAc4FloQvfGHNE2veEc34Fy9+Bhf+MdDQmAppM/qpaA9wGvI9L3K+o6hIRGSMiY7zJegBLRGQ5rlXQHd68M4DXgLnAIm99T4T8WxhjDt/pt0PnATD5F7CzNNLRmDATjcJDvpKSEp09225FN6bZbV8N4wa5R0ReO9F1D21ikojMUdWSYKe3v7QxrVluVzj/f92NX7OfjnQ0Jows+RvT2pXcBMeeC1Pus77/WxFL/sa0diIw8jHXCmjiGKirjXREJgws+RtjIKsTjPgzlM6Ezx6JdDQmDCz5G2Ocky6HniPh49+5B8CYFs2SvzHGEYEL/+KeBTxxDNTsi3REphlZ8jfGHJDeFi5+FDYvhqkPRjoa04ws+RtjDnb8cDj5WvjsYfh6RqSjMc3Ekr8x5lAX/B6yCuHNMVBVEeloTDOw5G+MOVRKFlwy1t0B/MEDkY7GNANL/saYwIoHw4BbYdaT8NW/Ix2NCbGESAdgjIli594Pqz50j34c9nt3RJCc7d5TsiE5CxLtsdyxyJK/MaZhialw6Xj4+4Xw6g2Bp4lPcpVASpb3nn1wJVFflpIN6flw7BCITwzv9zCHsORvjGlcwSnw06VQvhH2lUNlufe+070OGVcOuzcfGFe1+9DlXfYktD02Mt/HAJb8jTHBSMt1ryNRW+MqgX3lrunoez+H8YNh+P+5JqUS6EmxprnZBV9jTPOKT3AVR5si6HMF/NfnUNAPJt0Gr1wPe7ZHOsJWyZK/MSa8sgvh+klw3v/Aivdg3OnueQImrCz5G2PCLy4OzrgDbvkQkjJgwkh4/1fWn1AYWfI3xkROp77ww2lQcjN88Rg8eS5sWR7pqFqFoJK/iAwTkRUiskpE7glQ3kZEJorIQhGZKSK9fMpyROQ1EVkuIstEZGAov4AxJsYlpcFFD8FVL8OuTfDEWTDzSYjC54u3JE0mfxGJBx4HhgM9gatEpKffZL8E5qtqb+B6wPdpEI8A/1LVE4A+wLJQBG6MaWGOH+4uBhcNgsl3wYujYPeWSEfVYgWz598fWKWqq1W1CngZGOk3TU/gIwBVXQ4UiUh7EckCzgSe9sqqVHVHqII3xrQwme3hmtdg+B9h9ScwdiB8+X6ko2qRgkn+BcB6n8+l3jhfC4DLAESkP3AMUAh0BcqAv4vIPBF5SkTSA61EREaLyGwRmV1WVnaYX8MY02KIwGmj4YefQGYHdwTw7s+gak+kI2tRgkn+ge7A8D8Z9yDQRkTmA7cD84Aa3E1k/YBxqnoyUAEccs0AQFWfUNUSVS3Jz88PMnxjTIvVrgf84N8w8DaY9RQ8cTZsWhjpqFqMYJJ/KdDZ53MhsNF3AlUtV9Xvq2pf3Dn/fGCNN2+pqtY/EeI1XGVgjDFNS0iGC34L1010XUc8OQQ+exTq6iIdWcwLJvnPArqLSLGIJAFXApN8J/Ba9CR5H28BpnkVwjfAehE53is7F1gaotiNMa3FsUPg1i/guAvgg/tgwsXwzeJIRxXTmkz+qloD3Aa8j2up84qqLhGRMSIyxpusB7BERJbjWgXd4bOI24EXRGQh0Bf4XQjjN8a0Fmm5cMXzcPFfYdMCGH8G/PM62Lwk0pHFJNEobEtbUlKis2fPjnQYxphotfdb+GIsTB8HVbug50g46x5o798KvfUQkTmqWhLs9HaHrzEm9qS2gSG/gjsXwpk/h1X/hnED4ZUbYIvdShQMS/7GmNiVlgtD/ttVAoPvck8dGzsQXr3RuologiV/Y0zsS8uFc++DOxfB4J/Cyg9g7AB47SarBBpgyd8Y03Kk5brnDt+5CAb9xN0dPHYAvHYzlK2IdHRRxZK/MablScuFoQ/AHQth0J3uuQGPnwav3wJlX0Y6uqhgyd8Y03Klt4Whv3bXBM64A5ZPhrGnwes/gK0rIx1dRFlTT2NM61GxFT5/1HUZXVMJxw1zLYeOVptiOOvnR7+co3C4TT3tAe7GmNYjPc89PnLg7a4SWPoW6FF2FVFTCRVlrkvqDr2anj5KWPI3xrQ+Gflw/v+619Hasx3+1B0W/jOmkr+d8zfGmKORlgvdz4dFr0FdbaSjCZolf2OMOVq9R8GujbD200hHEjRL/sYYc7SOGwbJWbDwlUhHEjRL/sYYc7QSU6Hnxe4CcvXeSEcTFEv+xhgTCr2vcD2Mrpgc6UiCYsnfGGNC4ZhBkFUQM6d+LPkbY0woxMXBSZe7nkUrtkY6miZZ8jfGmFDpfQXU1cCSiZGOpEmW/I0xJlTanwjte7kbvqKcJX9jjAml3qOgdBZs+yrSkTQqqOQvIsNEZIWIrBKRewKUtxGRiSKyUERmikgvv/J4EZknIu+EKnBjjIlKvS4HBBa9GulIGtVk8heReOBxYDjQE7hKRPyfkvxLYL6q9gauBx7xK78DsAdrGmNavuwCKB7sTv1EYa/J9YLZ8+8PrFLV1apaBbwMjPSbpifwEYCqLgeKRKQ9gIgUAhcCT4UsamOMiWa9r4Dtq2HDnEhH0qBgkn8BsN7nc6k3ztcC4DIAEekPHAMUemUPA78AGu03VURGi8hsEZldVlYWRFjGGBOlenwHElKi+sJvMMlfAozzP5Z5EGgjIvOB24F5QI2IXARsUdUmqz9VfUJVS1S1JD8/P4iwjDEmSqVku/79F78OtdWRjiagYJJ/KdDZ53MhsNF3AlUtV9Xvq2pf3Dn/fGANcAZwsYisxZ0uGiIiz4cgbmOMiW69r4A92+Crf0c6koCCSf6zgO4iUiwiScCVwCTfCUQkxysDuAWY5lUI96pqoaoWefP9W1WvDWH8xhgTnY49F1Jzo/bUT5NP8lLVGhG5DXgfiAeeUdUlIjLGKx8P9AAmiEgtsBS4uRljNsaY6JeQBL0ug3nPQ2U5pGRFOqKD2APcjTGmuayfBU8PhZFj4eRrmnVVh/sAd7vD1xhjmkthCbQpjspTP5b8jTGmuYi4C79rpkH5xqanDyNL/sYY05x6jwLUPeA9iljyN8aY5tT2WCgoibqHvFjyN8aY5tb7Cti8CDYviXQk+1nyN8aY5tbrMpD4qNr7t+RvjDHNLT0Pug113TzXNdrNWdhY8jfGmHDoPQrKN8C6zyIdCWDJ3xhjwuP4EZCUETVt/i35G2NMOCSlQY+LYelbUF0Z6Wgs+RtjTNj0HgX7yuHLf0U6Ekv+xhgTNsVnQkaHqGj1Y8nfGGPCJS4eTrocVk6BPdsjG0pE126MMa1N7yugrhqWvBHRMCz5G2NMOHU4CfJ7RPzUjyV/Y4wJJxF34Xf9DNi+JmJhWPI3xphwO+l77n3RqxELwZK/McaEW05nOGaQu+ErQk9TtORvjDGR0HsUbFsFG+dGZPWW/I0xJhJ6joT45Ihd+A0q+YvIMBFZISKrROSeAOVtRGSiiCwUkZki0ssb31lEPhaRZSKyRETuCPUXMMaYmJSaA8cPc0/4qq0O++qbTP4iEg88DgwHegJXiUhPv8l+CcxX1d7A9cAj3vga4Geq2gMYAPwowLzGGNM69b4C9myF1VPDvupg9vz7A6tUdbWqVgEvAyP9pukJfASgqsuBIhFpr6qbVHWuN34XsAwoCFn0xhgTy7qdB6ltItLTZzDJvwBY7/O5lEMT+ALgMgAR6Q8cAxT6TiAiRcDJwIxAKxGR0SIyW0Rml5WVBRW8McbEtIQkOPFSWPYO7NsV1lUHk/wlwDj/tkkPAm1EZD5wOzAPd8rHLUAkA3gduFNVywOtRFWfUNUSVS3Jz88PJnZjjIl9va+Amr2w/N2wrjaY5F8KdPb5XAhs9J1AVctV9fuq2hd3zj8fWAMgIom4xP+Cqka2MwtjjIk2nU+DnC5hP/UTTPKfBXQXkWIRSQKuBCb5TiAiOV4ZwC3ANFUtFxEBngaWqepDoQzcGGNaBBG39796Kuz6JmyrbTL5q2oNcBvwPu6C7SuqukRExojIGG+yHsASEVmOaxVU36TzDOA6YIiIzPdeI0L+LYwxJpadNAq0zjX7DBPRCN1a3JiSkhKdPXt2pMMwxpjweeJsqKuFMf85otlFZI6qlgQ7vd3ha4wx0aDkZig8FWqqwrK6hLCsxRhjTOP6XedeYWJ7/sYY0wpZ8jfGmFbIkr8xxrRClvyNMaYVsuRvjDGtkCV/Y4xphSz5G2NMK2TJ3xhjWqGo7N5BRMqAdUc4ex6wNYThhEOsxRxr8YLFHC6xFnOsxQsNx3yMqgbdH35UJv+jISKzD6d/i2gQazHHWrxgMYdLrMUca/FC6GK20z7GGNMKWfI3xphWqCUm/yciHcARiLWYYy1esJjDJdZijrV4IUQxt7hz/sYYY5rWEvf8jTHGNMGSvzHGtEIxmfxFZJiIrBCRVSJyT4ByEZFHvfKFItIvEnH6xNNZRD4WkWUiskRE7ggwzdkistPnWcf3RyJWv5jWisgiL55DnqsZhdv5eJ/tN19EykXkTr9pIr6dReQZEdkiIot9xuWKyAcistJ7b9PAvI3+9sMc8x9FZLn3t58oIjkNzNvo7yiM8f5aRDY09TzxKNvG//SJd62IzG9g3sPfxqoaUy8gHvgK6AokAQuAnn7TjADeAwQYAMyIcMwdgX7ecCbwZYCYzwbeifT29YtpLZDXSHlUbecAv5NvcDe+RNV2Bs4E+gGLfcb9AbjHG74H+L8GvlOjv/0wx3w+kOAN/1+gmIP5HYUx3l8DdwXxu4mabexX/mfg/lBt41jc8+8PrFLV1apaBbwMjPSbZiQwQZ3pQI6IdAx3oPVUdZOqzvWGdwHLgIJIxRNCUbWd/ZwLfKWqR3qneLNR1WnAdr/RI4HnvOHngEsCzBrMb79ZBIpZVaeoao33cTpQGI5YgtHANg5GVG3jeiIiwCjgpVCtLxaTfwGw3udzKYcm0mCmiQgRKQJOBmYEKB4oIgtE5D0ROTG8kQWkwBQRmSMiowOUR+12Bq6k4f8o0badAdqr6iZwOwtAuwDTRPP2vgl3FBhIU7+jcLrNO031TAOn1qJ1Gw8GNqvqygbKD3sbx2LylwDj/NurBjNN2IlIBvA6cKeqlvsVz8WdougD/BV4M8zhBXKGqvYDhgM/EpEz/cqjdTsnARcDrwYojsbtHKxo3d6/AmqAFxqYpKnfUbiMA44F+gKbcKdR/EXlNgauovG9/sPexrGY/EuBzj6fC4GNRzBNWIlIIi7xv6Cqb/iXq2q5qu72hicDiSKSF+Yw/WPa6L1vASbiDol9Rd129gwH5qrqZv+CaNzOns31p8y89y0Bpom67S0iNwAXAdeod/LZXxC/o7BQ1c2qWquqdcCTDcQRjds4AbgM+GdD0xzJNo7F5D8L6C4ixd4e3pXAJL9pJgHXe61RBgA76w+pI8E7X/c0sExVH2pgmg7edIhIf9zfZlv4ojwknnQRyawfxl3cW+w3WVRtZx8N7iVF23b2MQm4wRu+AXgrwDTB/PbDRkSGAXcDF6vqngamCeZ3FBZ+16MubSCOqNrGnqHAclUtDVR4xNs4HFexm+Gq+Ahci5mvgF9548YAY7xhAR73yhcBJRGOdxDu0HEhMN97jfCL+TZgCa51wXTg9AjH3NWLZYEXV9RvZy+mNFwyz/YZF1XbGVcxbQKqcXuaNwNtgY+Ald57rjdtJ2Cyz7yH/PYjGPMq3Pnx+t/0eP+YG/odRSjef3i/04W4hN4x2rexN/7Z+t+vz7RHvY2tewdjjGmFYvG0jzHGmKNkyd8YY1ohS/7GGNMKWfI3xphWyJK/Mca0Qpb8jTGmFbLkb4wxrdD/B0oRbkaKSCU+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_scores = sorted(abs(bow_tuned_gbr.cv_results_['mean_test_score']), reverse = True)\n",
    "train_scores = sorted(abs(bow_tuned_gbr.cv_results_['mean_train_score']), reverse = True)\n",
    "\n",
    "plt.plot(test_scores, label='test')\n",
    "plt.plot(train_scores, label='train')\n",
    "plt.title('GBR GridSearch Validation Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bow GBR': array([1.98035442, 1.74065043, 1.97053596, 2.01026861, 1.7050703 ,\n",
       "        1.4336976 , 1.51837127, 2.00083402, 1.81927491, 1.91329422]),\n",
       " 'emb GBR': array([2.4285899 , 2.91262041, 2.62344132, 2.76556642, 2.47686792,\n",
       "        2.27430404, 2.32861419, 2.40971533, 2.53560754, 2.4352243 ]),\n",
       " 'bow LR': array([1.18956767, 1.45965528, 1.42917966, 1.39490569, 1.1646576 ,\n",
       "        1.27190421, 1.20866113, 1.20286103, 1.13738861, 1.22946898]),\n",
       " 'emb LR': array([0.60412894, 0.67672182, 0.67283618, 0.74049943, 0.6138729 ,\n",
       "        0.65476527, 0.66976956, 0.68931168, 0.58531628, 0.67472118]),\n",
       " 'Tuned GBR emb': array([1.04719986, 1.05021414, 1.03938978, 1.03617063, 1.01402188,\n",
       "        1.0192623 , 1.03324242, 1.03613458, 0.97617438, 0.97338644,\n",
       "        0.92558816, 0.91470772, 1.02096128, 1.01933199, 0.92920536,\n",
       "        0.91446632, 0.93382934, 0.87761503]),\n",
       " 'Tuned GBR bow': array([1.05444347, 1.05261295, 1.04560636, 1.04804068, 1.04293311,\n",
       "        1.04264232, 1.05528043, 1.04804591, 1.02753862, 1.02877832,\n",
       "        1.0267149 , 1.01354981, 1.04233855, 1.04270539, 1.02106821,\n",
       "        1.01369505, 1.03692072, 1.01450416])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_valid_RMSE_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE score arrays returned from each training step are not in the same length. Populate Mean value in each RMSE score array to make sure they are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of bow GBR RMSE array: 10\n",
      "Length of emb GBR RMSE array: 10\n",
      "Length of bow LR RMSE array: 10\n",
      "Length of emb LR RMSE array: 10\n",
      "Length of Tuned GBR emb RMSE array: 18\n",
      "Length of Tuned GBR bow RMSE array: 18\n"
     ]
    }
   ],
   "source": [
    "for key in model_valid_RMSE_comparisons.keys(): \n",
    "    # Print length of each RMSE score array\n",
    "    print(f\"Length of {key} RMSE array: {len(model_valid_RMSE_comparisons[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model_valid_RMSE_comparisons.keys():    \n",
    "    arr = model_valid_RMSE_comparisons[key]\n",
    "    arr_len = len(arr)\n",
    "    # if the length is not the same\n",
    "    if  arr_len < len(bow_tuned_gbr.cv_results_['mean_test_score']):\n",
    "        # populate a temp array to store mean value of each RMSE array\n",
    "        add_mean = [arr.mean() for i in range(len(bow_tuned_gbr.cv_results_['mean_test_score']) - arr_len)] \n",
    "        # append the mean of RMSE to the existing RMSE array\n",
    "        model_valid_RMSE_comparisons[key] = np.append(arr, add_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_df = pd.DataFrame.from_dict(model_valid_RMSE_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   3.0s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   2.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   4.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   3.4s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   3.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   3.5s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.8s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   3.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   3.2s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.4s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   2.0s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   2.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   1.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   2.8s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   2.0s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   4.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   7.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   3.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   8.0s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   3.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   7.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.4s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   2.0s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.2s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.2s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.0s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   2.0s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.2s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   2.0s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   4.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   3.4s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   3.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   3.5s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   4.0s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   3.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   2.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   1.6s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   1.8s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.2s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.2s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   2.6s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.8s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   3.5s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.4s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.4s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   4.0s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   8.2s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   3.4s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   7.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   2.1s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.0s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   1.6s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   1.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.2s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   2.7s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.4s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   3.5s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   7.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   4.0s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.4s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.4s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.4s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=True; total time=   0.4s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.8s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   3.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   7.2s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.0s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   2.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.0s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   1.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   2.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.7s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   4.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   7.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   3.4s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   7.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=False; total time=   3.9s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.4s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   2.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   2.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   2.0s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   2.2s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=False; total time=   0.8s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.8s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   3.5s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   7.8s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=True; total time=   3.4s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   8.1s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   3.3s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   7.2s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=3, subsampling_of_rows=True; total time=   0.3s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=True; total time=   1.9s\n",
      "[CV] END learning_rate=0.003, num_base_estimators=30, subsampling_of_rows=False; total time=   2.9s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=15, subsampling_of_rows=False; total time=   1.5s\n",
      "[CV] END learning_rate=0.015, num_base_estimators=30, subsampling_of_rows=False; total time=   2.9s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=15, subsampling_of_rows=True; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=True; total time=   2.0s\n",
      "[CV] END learning_rate=0.03, num_base_estimators=30, subsampling_of_rows=False; total time=   2.2s\n"
     ]
    }
   ],
   "source": [
    "# Making a plot\n",
    "bp = plot.boxplot(RMSE_df) \n",
    "\n",
    "plot.ylabel('RMSE')\n",
    "plot.xticks([1, 2, 3, 4, 5, 6],\n",
    "           RMSE_df.columns,\n",
    "           rotation=45)\n",
    "# display the plot\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a short (no more than 300 words) reflection on the different models trained as part of this work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The GradientBoosting Regressor (GBR) model achieved **smaller** Root Mean Square Error (RMSE) when it was trained on the bag of words (BOW) representation dataset. The model returned **higher RMSE** when it was trained on the sentence embedding (EMB) representation dataset. Therefore, we learned that given different inpute features, the same machine learning (ML) model would have different performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Our baseline model, Linear Regressor (LR) model, achieved **the smallest RMSE** on the EMB representation data among all the evaluated models in this project. It indicates that the complicated ML model does not always give better performance than some simple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Hyperparameter Tuning process significantly improved the performance of the GBR model, because the evaluation metrics - Root Mean Square Error (RMSE), **dropped significantly** after GridSearchCV function was executed. On both the BOW representation dataset and the EMB representation dataset, Hyperparameter Tuning achieved **smaller RMSE**. It suggests that Hyperparameter Tuning process is quite necessary for searching the best hyperparameter of a ML model to achieve the best performance. A ML model with default hyperparameters does not give good performance generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In this project, we only do tuning on three hyperparameters for the GBR model. In fact, this model has more hyperparameters that can also be fine tuned, e.g. subsampling ratio, max tree depth, min samples leaf, etc. If we perform tuning on these hyperparameters as well, we believe the performance of GBR model would be improved too. Due to the limitation of time, we leave this for our future work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The two 'GBR GridSearch Validation Curve' plots show that the Grid Search scores did not reach the global optimization, because the RMSE values are still decreasing. We only set-up 18 candidates for the GBR model to do Grid Search. There are more candicates we can set to let the model achieve the global optimization. Because this operation costs a lot of time to do, we leave it for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Tuning the GBR model on BOW representation data gives similar mean value of RMSE with tuning model on EMB representation data. Because the  GridSearchCV function is a gradient algorithm which tries to find the smallest training error globally, the performance of model on different input feature does not vary a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
